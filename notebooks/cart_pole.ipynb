{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\n",
    "    \"figure\",\n",
    "    autolayout=True,\n",
    "    figsize=(12, 6\n",
    "    \n",
    "    ),\n",
    "    titlesize=18,\n",
    "    titleweight='bold',\n",
    ")\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"small\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=16,\n",
    "    titlepad=10,\n",
    ")\n",
    "plot_params = dict(\n",
    "    color=\"0.75\",\n",
    "    style=\".-\",\n",
    "    markeredgecolor=\"0.25\",\n",
    "    markerfacecolor=\"0.25\",\n",
    "    legend=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole\n",
    "\n",
    "https://gymnasium.farama.org/environments/classic_control/cart_pole/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Random Actions:\n",
    "\n",
    "**Objective**: Understand the environment and its dynamics.\n",
    "\n",
    "**Task**:\n",
    "- Initialize the environment.\n",
    "- At each step, choose an action randomly.\n",
    "- Observe the consequences of the action in terms of the next state, reward, and whether the episode has ended.\n",
    "\n",
    "**Expected Outcome**: \n",
    "- The pole will likely fall quickly.\n",
    "- Gain an intuitive understanding of the environment's dynamics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 50\n",
    "episode_scores = {}\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    observation, _ = env.reset() \n",
    "    episode_reward = 0 \n",
    "    while True:\n",
    "        env.render()\n",
    "\n",
    "        # actions are 0 or 1 (move left or right)\n",
    "        action = env.action_space.sample()  \n",
    "\n",
    "        # observation is a 4-tuple of floats: cart_position, cart_velocity, pole_angle, pole_angular_velocity \n",
    "        # info is usually(?) empty for cartpole\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        episode_reward += reward  \n",
    "        \n",
    "        if terminated or truncated:\n",
    "            episode_scores[f\"episode_{episode}\"] = episode_reward\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_score = sum(episode_scores.values()) / len(episode_scores)\n",
    "mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward is +1 for every step, so the scores are just the episode lengths\n",
    "plot_data = pd.DataFrame(episode_scores.values())\n",
    "sns.lineplot(plot_data)\n",
    "plt.title(f'Random actions mean score: {mean_score:.2f})')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0,500)  # truncation happens at 500 steps\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tabular Q-learning (if state space is discretized):\n",
    "\n",
    "**Objective**: Grasp the concept of value-based learning.\n",
    "\n",
    "**Task**:\n",
    "- Discretize the state space into bins.\n",
    "- Create a Q-table with state-action pairs.\n",
    "- Implement the Q-learning algorithm to update Q-values.\n",
    "- Choose actions using an epsilon-greedy policy.\n",
    "\n",
    "**Expected Outcome**:\n",
    "- Initial unstable results, but with enough episodes and appropriate hyperparameters, you should see improvements in the agent's performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_state(observation, bins):\n",
    "    \n",
    "    \"\"\"Discretize a continuous observation into discrete values.\"\"\"\n",
    "    # Define the range for each observation value\n",
    "    state_bounds = [(-2.4, 2.4), (-3, 3), (-0.21, 0.21), (-3, 3)]  # OK for CartPole-v1?\n",
    "\n",
    "    discrete_observation = []\n",
    "    \n",
    "    for i in range(len(observation)):\n",
    "        # Clip the observation value within defined bounds\n",
    "        obs = np.clip(observation[i], state_bounds[i][0], state_bounds[i][1])\n",
    "        \n",
    "        # Scale the observation to the range [0, 1]\n",
    "        scaled_obs = (obs - state_bounds[i][0]) / (state_bounds[i][1] - state_bounds[i][0])\n",
    "        \n",
    "        # Discretize using the provided bins and append to the discrete state\n",
    "        discrete_observation.append(min(int(scaled_obs * bins[i]), bins[i] - 1))\n",
    "    \n",
    "    return tuple(discrete_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "ALPHA = 0.1  # Learning rate\n",
    "GAMMA = 0.99  # Discount factor\n",
    "EPSILON = 0.1  # Exploration rate\n",
    "BINS = [24, 24, 24, 24]  # Number of bins for discretization\n",
    "NUM_EPISODES = 10_000\n",
    "\n",
    "# Initialize Q-table with zeros\n",
    "q_table = np.zeros(BINS + [env.action_space.n])\n",
    "\n",
    "episode_scores = {}\n",
    "\n",
    "# Q-learning algorithm\n",
    "for episode in range(NUM_EPISODES):\n",
    "    observation, _ = env.reset()\n",
    "    discrete_state = discretize_state(observation, BINS)\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while not terminated and not truncated:\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.uniform(0, 1) < EPSILON:\n",
    "            action = env.action_space.sample()  # Explore\n",
    "        else:\n",
    "            action = np.argmax(q_table[discrete_state])  # Exploit\n",
    "\n",
    "        # Take action and get new state and reward\n",
    "        new_observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        new_discrete_state = discretize_state(new_observation, BINS)\n",
    "        \n",
    "        # Q-learning update\n",
    "        if not terminated and not truncated:\n",
    "            max_future_q = np.max(q_table[new_discrete_state])\n",
    "            current_q    = q_table[discrete_state + (action,)]\n",
    "            new_q        = (1 - ALPHA) * current_q + ALPHA * (reward + GAMMA * max_future_q)\n",
    "            q_table[discrete_state + (action,)] = new_q\n",
    "\n",
    "        else:\n",
    "            episode_scores[f\"episode_{episode}\"] = episode_reward\n",
    "\n",
    "        episode_reward += reward\n",
    "        discrete_state = new_discrete_state\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode: {episode}, Total Reward: {episode_reward}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_score = sum(episode_scores.values()) / len(episode_scores)\n",
    "mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = pd.DataFrame(episode_scores.values())\n",
    "sns.lineplot(plot_data.values[0::100]) # plot every 100th value so it's not too crowded\n",
    "plt.title(f'Tabular Q-learning (mean score: {mean_score:.2f}))')\n",
    "plt.xlabel('Episode (x100)')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0,500)  # truncation happens at 500 steps\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Policy Gradient using Neural Networks:\n",
    "\n",
    "**Objective**: Transition from tabular methods to function approximators like neural networks.\n",
    "\n",
    "**Task**:\n",
    "- Use a neural network to estimate the policy.\n",
    "- Implement the REINFORCE algorithm or a similar vanilla policy gradient method.\n",
    "- Update the policy based on the received rewards.\n",
    "\n",
    "**Expected Outcome**:\n",
    "- The agent will learn to balance the pole for longer durations.\n",
    "- Understanding of how neural networks can be used in RL.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.01\n",
    "EPISODES = 100\n",
    "\n",
    "# Initialize environment and policy network\n",
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(state_size,))\n",
    "x = Dense(24, activation='relu')(inputs)\n",
    "x = Dense(24, activation='relu')(x)\n",
    "outputs = Dense(n_actions, activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# we have a custom training loop which doesn't fit with the model.compile() pattern\n",
    "optimizer = Adam(learning_rate=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_scores = {}\n",
    "for episode in range(1, EPISODES + 1):\n",
    "    state, _ = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    episode_states, episode_actions, episode_rewards = [], [], []\n",
    "\n",
    "    while not terminated and not truncated:\n",
    "        # Forward pass\n",
    "        action_prob = model.predict(state.reshape(1, -1), verbose=0)[0]\n",
    "        action = np.random.choice(n_actions, p=action_prob)\n",
    "\n",
    "        # Take action\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        # Store state, action and reward\n",
    "        episode_states.append(state)\n",
    "        episode_actions.append(action)\n",
    "        episode_rewards.append(reward)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    episode_scores[f\"episode_{episode}\"] = np.sum(episode_rewards)\n",
    "\n",
    "    # Compute discounted rewards\n",
    "    discounted_rewards = []\n",
    "    cumulative_reward = 0\n",
    "    for reward in reversed(episode_rewards):\n",
    "        cumulative_reward = reward + 0.99 * cumulative_reward\n",
    "        discounted_rewards.insert(0, cumulative_reward)\n",
    "    discounted_rewards = np.array(discounted_rewards)\n",
    "    discounted_rewards = (discounted_rewards - np.mean(discounted_rewards)) / (np.std(discounted_rewards) + 1e-9)\n",
    "\n",
    "    # Compute loss values and perform a gradient step\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "        probs = model(np.vstack(episode_states))\n",
    "        indices = tf.range(0, tf.shape(probs)[0]) * tf.shape(probs)[1] + episode_actions\n",
    "        chosen_probs = tf.gather(tf.reshape(probs, [-1]), indices)\n",
    "        loss = -tf.reduce_mean(tf.math.log(chosen_probs) * discounted_rewards)\n",
    "\n",
    "    # Compute gradients and update model weights\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    print(f\"Episode: {episode}, Total Reward: {np.sum(episode_rewards)}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_score = sum(episode_scores.values()) / len(episode_scores)\n",
    "mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = pd.DataFrame(episode_scores.values())\n",
    "sns.lineplot(plot_data.values) \n",
    "plt.title(f'Policy gradient using NN (mean score: {mean_score:.2f}))')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0,500)  # truncation happens at 500 steps\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "# run the trained model\n",
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "state, _ = env.reset()\n",
    "state = np.reshape(state, [1, 4])\n",
    "\n",
    "terminated = False\n",
    "truncated = False\n",
    "\n",
    "while not terminated and not truncated:\n",
    "    env.render()\n",
    "\n",
    "    action_prob = model.predict(state)\n",
    "    action = np.argmax(action_prob[0])\n",
    "\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    next_state = np.reshape(next_state, [1, 4])\n",
    "    state = next_state\n",
    "\n",
    "    # Adding a small sleep to make the rendering smoother\n",
    "    time.sleep(0.01)\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Deep Q-learning:\n",
    "\n",
    "**Objective**: Learn how to use deep neural networks in value-based methods.\n",
    "\n",
    "**Task**:\n",
    "- Use a neural network as a Q-function approximator.\n",
    "- Implement the DQN algorithm with experience replay and target networks.\n",
    "\n",
    "**Expected Outcome**:\n",
    "- Improved stability compared to simple Q-learning due to the use of neural networks and experience replay.\n",
    "- Understanding of challenges like overestimation bias in Q-learning and the need for techniques like target networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import gymnasium as gym\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.001\n",
    "GAMMA = 0.99\n",
    "MEMORY_SIZE = 10_000\n",
    "BATCH_SIZE = 32\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.01\n",
    "EXPLORATION_DECAY = 0.995\n",
    "\n",
    "NUM_EPISODES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Q-network model\n",
    "input = layers.Input(shape=(4,))\n",
    "x = layers.Dense(24, activation='relu')(input)\n",
    "x = layers.Dense(24, activation='relu')(x)\n",
    "output = layers.Dense(2, activation='linear')(x)\n",
    "\n",
    "model = keras.Model(inputs=input, outputs=output)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='mse'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize other variables\n",
    "memory = []\n",
    "exploration_rate = EXPLORATION_MAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_scores = {}\n",
    "for episode in range(1, NUM_EPISODES+1):  \n",
    "    state, _ = env.reset()\n",
    "    state = np.reshape(state, [1, 4])\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not terminated and not truncated:\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.rand() < exploration_rate:\n",
    "            action = random.randrange(2)\n",
    "        else:\n",
    "            q_values = model.predict(state)\n",
    "            action = np.argmax(q_values[0])\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, 4])\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Store experience in memory\n",
    "        memory.append((state, action, reward, next_state, terminated, truncated))\n",
    "        if len(memory) > MEMORY_SIZE:\n",
    "            memory.pop(0)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        # Train the Q-network\n",
    "        if len(memory) >= BATCH_SIZE:\n",
    "            minibatch = random.sample(memory, BATCH_SIZE)\n",
    "            for state, action, reward, next_state, terminated, truncated in minibatch:\n",
    "                if terminated or truncated:\n",
    "                    q_update = reward\n",
    "                else:\n",
    "                    q_values_next = model.predict(next_state, verbose=0)\n",
    "                    q_update = reward + GAMMA * np.max(q_values_next)\n",
    "                q_values = model.predict(state, verbose=0)\n",
    "                q_values[0][action] = q_update\n",
    "                model.fit(state, q_values, verbose=0)\n",
    "\n",
    "    episode_scores[f\"episode_{episode}\"] = episode_reward\n",
    "    print(f\"Episode: {episode}, Total Reward: {episode_reward}\")\n",
    "\n",
    "    # Exploration rate decay\n",
    "    if exploration_rate > EXPLORATION_MIN:\n",
    "        exploration_rate *= EXPLORATION_DECAY\n",
    "\n",
    "    # You can add code here to print episode statistics, save models, etc.\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_score = sum(episode_scores.values()) / len(episode_scores)\n",
    "mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = pd.DataFrame(episode_scores.values())\n",
    "sns.lineplot(plot_data.values) \n",
    "plt.title(f'Policy gradient using Deep Q-Learning (mean score: {mean_score:.2f}))')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0,500)  # truncation happens at 500 steps\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Advanced DQN Variants:\n",
    "\n",
    "**Objective**: Dive deeper into challenges and improvements in value-based deep RL.\n",
    "\n",
    "**Task**:\n",
    "- Explore algorithms like Double DQN, Dueling DQN, and Prioritized Experience Replay.\n",
    "- Integrate these techniques into your DQN implementation.\n",
    "\n",
    "**Expected Outcome**:\n",
    "- Improved performance and stability.\n",
    "- Comprehensive understanding of challenges in deep Q-learning and the methodologies to mitigate them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.001\n",
    "MEMORY_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.01\n",
    "EXPLORATION_DECAY = 0.995\n",
    "EPISODES = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "observation_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize memory and models\n",
    "memory = deque(maxlen=MEMORY_SIZE)\n",
    "model = Sequential([Dense(24, input_shape=(observation_space,), activation=\"relu\"),\n",
    "                    Dense(24, activation=\"relu\"),\n",
    "                    Dense(action_space, activation=\"linear\")])\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = Sequential([Dense(24, input_shape=(observation_space,), activation=\"relu\"),\n",
    "                           Dense(24, activation=\"relu\"),\n",
    "                           Dense(action_space, activation=\"linear\")])\n",
    "target_model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "exploration_rate = EXPLORATION_MAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "episode_scores = {}\n",
    "for episode in range(EPISODES):\n",
    "    state, _ = env.reset()\n",
    "    state = np.reshape(state, [1, observation_space])\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    \n",
    "    while not terminated and not truncated:\n",
    "        if np.random.rand() < exploration_rate:\n",
    "            action = random.randrange(action_space)\n",
    "        else:\n",
    "            q_values = model.predict(state)\n",
    "            action = np.argmax(q_values[0])\n",
    "        \n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, observation_space])\n",
    "\n",
    "        memory.append((state, action, reward, next_state, terminated, truncated))\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if len(memory) < BATCH_SIZE:\n",
    "            continue\n",
    "\n",
    "        # Double DQN Logic\n",
    "        minibatch = random.sample(memory, BATCH_SIZE)\n",
    "        for state, action, reward, next_state, terminated, truncated in minibatch:\n",
    "            if done:\n",
    "                q_update = reward\n",
    "            else:\n",
    "                q_values = model.predict(next_state)\n",
    "                best_action = np.argmax(q_values[0])\n",
    "                target_q_values = target_model.predict(next_state)\n",
    "                q_update = reward + GAMMA * target_q_values[0][best_action]\n",
    "            \n",
    "            q_values = model.predict(state)\n",
    "            q_values[0][action] = q_update\n",
    "            model.fit(state, q_values, verbose=0)\n",
    "\n",
    "    target_model.set_weights(model.get_weights())\n",
    "    exploration_rate *= EXPLORATION_DECAY\n",
    "    exploration_rate = max(EXPLORATION_MIN, exploration_rate)\n",
    "\n",
    "    episode_scores[f\"episode_{episode}\"] = episode_reward\n",
    "    print(f\"Episode: {episode}, Total Reward: {episode_reward}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_score = sum(episode_scores.values()) / len(episode_scores)\n",
    "mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = pd.DataFrame(episode_scores.values())\n",
    "sns.lineplot(plot_data.values) \n",
    "plt.title(f'Double SQN model (mean score: {mean_score:.2f}))')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0,500)  # truncation happens at 500 steps\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Actor-Critic Methods:\n",
    "\n",
    "**Objective**: Combine the benefits of value-based and policy-based methods.\n",
    "\n",
    "**Task**:\n",
    "- Implement a basic Actor-Critic model.\n",
    "- Extend it to methods like Deep Deterministic Policy Gradient (DDPG) or Proximal Policy Optimization (PPO), even though these might be overkill for CartPole.\n",
    "\n",
    "**Expected Outcome**:\n",
    "- A balanced understanding of how value and policy methods can be combined.\n",
    "- Familiarity with advanced RL algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "EPISODES = 100\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Actor model\n",
    "input_state = layers.Input(shape=(state_size,))\n",
    "x = layers.Dense(24, activation='relu')(input_state)\n",
    "output_probs = layers.Dense(n_actions, activation='softmax')(x)\n",
    "\n",
    "actor = Model(inputs=input_state, outputs=output_probs)\n",
    "\n",
    "actor.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "              loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Critic model\n",
    "x = layers.Dense(24, activation='relu')(input_state)\n",
    "value = layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "critic = Model(inputs=input_state, outputs=value)\n",
    "\n",
    "critic.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "               loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "episode_scores = {}\n",
    "for episode in range(1, EPISODES + 1):\n",
    "    state, _ = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    episode_reward = 0\n",
    "    while not terminated and not truncated:\n",
    "        episode_reward += 1\n",
    "        action_prob = actor.predict(state.reshape(1, -1), verbose=0)\n",
    "        action = np.random.choice(n_actions, p=action_prob.ravel())\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        target = reward + GAMMA * critic.predict(next_state.reshape(1, -1), verbose=0) * (not (terminated or truncated))\n",
    "        advantage = target - critic.predict(state.reshape(1, -1), verbose=0)\n",
    "        \n",
    "        # Update Critic\n",
    "        critic.fit(state.reshape(1, -1), target, verbose=0)\n",
    "        \n",
    "        # Update Actor\n",
    "        action_one_hot = np.zeros(n_actions)\n",
    "        action_one_hot[action] = 1\n",
    "        actor.fit(state.reshape(1, -1), action_one_hot.reshape(1, -1) * advantage, verbose=0)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    episode_scores[f\"episode_{episode}\"] = episode_reward\n",
    "    print(f\"Episode: {episode}, Total Reward: {episode_reward}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_score = sum(episode_scores.values()) / len(episode_scores)\n",
    "mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = pd.DataFrame(episode_scores.values())\n",
    "sns.lineplot(plot_data.values) \n",
    "plt.title(f'Actor-Critic model (mean score: {mean_score:.2f}))')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0,500)  # truncation happens at 500 steps\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
